{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.remove('/usr/local/lib/python3.6/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"reudce\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "origin_dim (InputLayer)      [(None, 18000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                900050    \n",
      "=================================================================\n",
      "Total params: 900,050\n",
      "Trainable params: 900,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_img (InputLayer)       [(None, 36000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 18000)        0           original_img[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 18000)        0           original_img[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reudce (Functional)             (None, 50)           900050      lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 50)           0           reudce[0][0]                     \n",
      "                                                                 reudce[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Square (TensorFlowO [(None, 50)]         0           subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [()]                 0           tf_op_layer_Square[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 900,050\n",
      "Trainable params: 900,050\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "input_len = 18000\n",
    "reduce_input = keras.Input(shape=(input_len), name=\"origin_dim\")\n",
    "reduce_output = layers.Dense(50, activation=\"relu\")(reduce_input)\n",
    "reduce = keras.Model(reduce_input, reduce_output, name=\"reudce\")\n",
    "reduce.summary()\n",
    "encoder_input = keras.Input(shape=(input_len * 2), name=\"original_img\")\n",
    "x1 = Lambda(lambda x: x[:,:input_len])(encoder_input)\n",
    "x2 = Lambda(lambda x: x[:,input_len:])(encoder_input)\n",
    "y1 = reduce(x1)\n",
    "y2 = reduce(x2)\n",
    "subtracted = keras.layers.Subtract()([y1, y2])\n",
    "square_layer =  tf.keras.backend.square(subtracted)\n",
    "sum_layer = tf.keras.backend.sum(square_layer)\n",
    "encoder = keras.Model(encoder_input, sum_layer, name=\"encoder\")\n",
    "encoder.summary()\n",
    "encoder.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "    },\n",
    "#    loss_weights=[1.0, 0.2],\n",
    ")\n",
    "keras.utils.plot_model(encoder, \"multi_input_and_output_model.png\", show_shapes=True)\n",
    "model = encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected  15743 feaures\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraphRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0mlist_dir_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mlist_child_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"child\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mchild_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m## REMOVE THIS TO RUN DATA FROM CHILD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mlist_child_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"child\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mchild_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m## REMOVE THIS TO RUN DATA FROM CHILD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mlist_child_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"child\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mchild_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m## REMOVE THIS TO RUN DATA FROM CHILD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mlist_child_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"child\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mchild_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m## REMOVE THIS TO RUN DATA FROM CHILD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mlist_child_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"child\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mchild_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_DFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m## REMOVE THIS TO RUN DATA FROM CHILD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_DFS\u001b[0;34m(self, parent, list_dir_node)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_child_trainable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_is_trained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_child_trainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_dir_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain_one_node\u001b[0;34m(self, node, list_child_data, list_dir_node)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparent_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparent_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                 \u001b[0mlist_child_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                 list_dir_node)\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-567112264f65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, node_id, node_data, child_data, list_dir_node)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m#X_test = X_test[:, selected_col]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_id_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import collections\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "PATH_CELL_ONTOLOGY = '/raidixshare_log-g/bbrowser/tung/cell_ontology/data/cell'\n",
    "PATH_CELL_TYPE_DATA = '/raidixshare_log-g/bbrowser/tung/Cell_type_data/'\n",
    "root = 'BTC000000'\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global_data = {}\n",
    "\n",
    "\n",
    "\n",
    "map_btc2name = {}\n",
    "for cell_type in os.listdir(PATH_CELL_ONTOLOGY):\n",
    "    if '.' in cell_type:\n",
    "        continue\n",
    "    with open(os.path.join(PATH_CELL_ONTOLOGY, cell_type, 'info.json'), 'r') as fin:\n",
    "        data = json.load(fin)\n",
    "    map_btc2name[cell_type] = data[\"name\"]\n",
    "\n",
    "def get_name_btc(list_btc):\n",
    "    return [map_btc2name[i] for i in list_btc]\n",
    "    \n",
    "\n",
    "\n",
    "class NodeRandomForest:\n",
    "    _number_train = 20000\n",
    "    _number_test =  20000\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def train(self, node_id, node_data, child_data, list_dir_node):\n",
    "        if 'BTC000229' != node_id:\n",
    "            return\n",
    "        \n",
    "        all_data = {}\n",
    "        for child in child_data:\n",
    "            if len(child_data[child][\"train\"]) + len(child_data[child][\"test\"]) <= 3:\n",
    "                continue\n",
    "            all_data[child] = child_data[child]\n",
    "        \n",
    "        if len(node_data[\"train\"]) != 0:\n",
    "            all_data[node_id] = node_data\n",
    "        \n",
    "        list_dir_node[node_id] = all_data\n",
    "        \n",
    "        X_train = []\n",
    "        Y_train = []\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "        study_id_train = []\n",
    "        study_id_test = []\n",
    "        \n",
    "        size_matrix = []\n",
    "        if not len(all_data) >= 2:\n",
    "            return\n",
    "\n",
    "        for node in all_data:\n",
    "            number_study = len(all_data[node][\"train\"])\n",
    "            sample_one_study = int(self._number_train / (number_study * len(all_data)))\n",
    "            for study_dir in all_data[node][\"train\"]:\n",
    "                with open(study_dir, 'rb') as fin:\n",
    "                    #print(study_dir)\n",
    "                    data = pickle.load(fin)\n",
    "                    #print([name for name in data])\n",
    "                    matrix = data['X']\n",
    "                    sampling = random.choices(range(len(matrix)), k=sample_one_study)\n",
    "                    X_train.append(matrix[np.array(sampling), :])\n",
    "                    Y_train.extend([node] * len(sampling))\n",
    "                    study_id_train.extend([study_dir] * len(sampling))\n",
    "                    #study_id_train.extend([study_dir.split('/')[-1]] * len(sampling))\n",
    "\n",
    "            number_study = len(all_data[node][\"test\"])\n",
    "            sample_one_study = int(self._number_test / (number_study * len(all_data)))\n",
    "            for study_dir in all_data[node][\"test\"]:\n",
    "                with open(study_dir, 'rb') as fin:\n",
    "                    data = pickle.load(fin)\n",
    "                    #print([name for name in data])\n",
    "                    matrix = data['X']\n",
    "                    size_matrix.append((len(matrix), node))\n",
    "                    sampling = random.choices(range(len(matrix)), k=sample_one_study)\n",
    "                    X_test.append(matrix[np.array(sampling), :])\n",
    "                    Y_test.extend([node] * len(sampling))\n",
    "                    study_id_test.extend([study_dir] * len(sampling))\n",
    "        \n",
    "        X_train = np.concatenate((X_train), axis=0)\n",
    "        #X_train = X_train[:, selected_col]\n",
    "        \n",
    "        col_sum = sum(X_train != 0) / X_train.shape[0]\n",
    "        pos = np.where(col_sum > 3.0 / (len(all_data) * 10) )[0]\n",
    "        print(\"Selected \", len(pos), 'feaures')\n",
    "        X_train = X_train[:,pos]\n",
    "        \n",
    "        \n",
    "        X_test = np.concatenate((X_test), axis=0)\n",
    "        #X_test = X_test[:,selected_col]\n",
    "        #X_test = X_test[:, selected_col]\n",
    "        X_test = X_test[:,pos]\n",
    "        print(Counter(study_id_train))\n",
    "        \n",
    "\n",
    "        le = preprocessing.LabelEncoder()                                                                                                                                         \n",
    "        le.fit(Y_train)\n",
    "        Y_train = le.transform(Y_train)\n",
    "        Y_test = le.transform(Y_test)\n",
    "        \n",
    "        le_study = preprocessing.LabelEncoder()\n",
    "        le_study.fit(study_id_train)\n",
    "        study_id_train = le_study.transform(study_id_train)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        clf = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(X_train.shape[1])),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "        clf.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        clf.fit(X_train, Y_train, epochs=10)\n",
    "        \n",
    "        probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "        predictions = probability_model.predict(test_images)\n",
    "        np.argmax(predictions)\n",
    "        '''\n",
    "        \n",
    "        global_data[\"X_train\"] = X_train\n",
    "        global_data[\"X_test\"] = X_test\n",
    "        global_data[\"Y_train\"] = Y_train\n",
    "        global_data[\"Y_test\"] = Y_test\n",
    "        global_data[\"study\"] = study_id_train\n",
    "        \n",
    "        \n",
    "        \n",
    "        clf = RandomForestClassifier(n_estimators= 90, max_depth=40,\n",
    "                                    max_leaf_nodes= max(Y_train + 1) * 20,\n",
    "                                    random_state=0, n_jobs = 30)\n",
    "        clf = clf.fit(X_train, Y_train)\n",
    "        \n",
    "        train_score = clf.score(X_train, Y_train)\n",
    "        test_score = clf.score(X_test, Y_test)\n",
    "        print(\"###########################################\")\n",
    "        print(node_id)\n",
    "        print(\"Score :\", train_score)\n",
    "        print(\"Test :\", test_score)\n",
    "        \n",
    "        \n",
    "        clf = RandomForest(1, max_depth=10)\n",
    "        clf.fit(X_train, Y_train, study_id_train)\n",
    "        \n",
    "        train_score = clf.score(X_train, Y_train)\n",
    "        test_score = clf.score(X_test, Y_test)\n",
    "        print(\"###########################################\")\n",
    "        print(node_id)\n",
    "        print(\"Score :\", train_score)\n",
    "        print(\"Test :\", test_score)\n",
    "        print(size_matrix)\n",
    "        confusion = confusion_matrix(Y_test, clf.predict(X_test))\n",
    "        \n",
    "        \n",
    "        with open(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'result.pkl'), 'wb') as fout:\n",
    "            pickle.dump({\n",
    "              \"train_score\": train_score,\n",
    "              \"test_score\": test_score,\n",
    "              \"confusion_matrix\": confusion,\n",
    "              \"label\": le.classes_\n",
    "            }, fout)\n",
    "        self.plot_confusion(node_id)\n",
    "        with open(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'model.pkl'), 'wb') as fout:\n",
    "            pickle.dump({\n",
    "                \"model\": clf,\n",
    "                \"le\": le\n",
    "            }, fout)\n",
    "            \n",
    "    def remove_result(self, node_id):\n",
    "        if not os.path.exists(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'result.pkl')):\n",
    "            return\n",
    "        os.remove(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'result.pkl'))\n",
    "        \n",
    "        \n",
    "    def plot_confusion(self, node_id):\n",
    "        \n",
    "        if not os.path.exists(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'result.pkl')):\n",
    "            return\n",
    "        with open(os.path.join(PATH_CELL_TYPE_DATA, node_id, 'result.pkl'), 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "            \n",
    "        print(node_id)\n",
    "        print(\"Train score \", data[\"train_score\"])\n",
    "        print(\"Test score\", data[\"test_score\"])\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        cax = ax.imshow(data[\"confusion_matrix\"], interpolation='nearest')\n",
    "        #fig.colorbar(cax)\n",
    "        ax.set_xticks(np.arange(len(data[\"label\"])))\n",
    "        ax.set_yticks(np.arange(len(data[\"label\"])))\n",
    "\n",
    "        ax.set_xticklabels(get_name_btc(data[\"label\"]))\n",
    "        ax.set_yticklabels(get_name_btc(data[\"label\"]))\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                rotation_mode=\"anchor\")\n",
    "\n",
    "        for i in range(len(data[\"label\"])):\n",
    "            for j in range(len(data[\"label\"])):\n",
    "                text = ax.text(j, i, data[\"confusion_matrix\"][i, j],\n",
    "                              ha=\"center\", va=\"center\", color=\"w\")\n",
    "                \n",
    "        ax.set_title(\"Confusion matrix\")\n",
    "        fig.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class GraphRandomForest:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def check_is_trained(self, node):\n",
    "        if os.path.exists(os.path.join(PATH_CELL_TYPE_DATA, node, 'result.pkl')):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def train_one_node(self, node, list_child_data, list_dir_node):\n",
    "        if not os.path.exists(os.path.join(PATH_CELL_TYPE_DATA, node)):\n",
    "            return\n",
    "        with open(os.path.join(PATH_CELL_TYPE_DATA, node, 'config.json'), 'r') as fin:\n",
    "            data = json.load(fin)\n",
    "        parent_train = [os.path.join(PATH_CELL_TYPE_DATA, node, 'data_index',x.replace('.hdf5', '.pkl')) for x in data['train']]\n",
    "        parent_test = [os.path.join(PATH_CELL_TYPE_DATA, node, 'data_index',x.replace('.hdf5', '.pkl')) for x in data['test']]\n",
    "        node_random_forest = NodeRandomForest()\n",
    "        node_random_forest.train(node, \n",
    "                                {\"train\": parent_train, \"test\": parent_test},\n",
    "                                list_child_data,\n",
    "                                list_dir_node)\n",
    "            \n",
    "\n",
    "    def train_DFS(self, parent, list_dir_node):\n",
    "        data_train, data_test = set(), set()\n",
    "        info_dir = os.path.join(PATH_CELL_ONTOLOGY, parent, 'info.json')\n",
    "        with open(info_dir, 'r') as fin:\n",
    "            info_graph = json.load(fin)    \n",
    "        if \"child\" not in info_graph:\n",
    "            info_graph[\"child\"] = []\n",
    "        \n",
    "        list_child_trainable = {}\n",
    "        for child in info_graph[\"child\"]:\n",
    "            child_train, child_test = self.train_DFS(child, list_dir_node)\n",
    "            \n",
    "            ## REMOVE THIS TO RUN DATA FROM CHILD\n",
    "            \n",
    "            #data_train = data_train | child_train\n",
    "            #data_test = data_test | child_test\n",
    "\n",
    "            if len(child_train) >= 2 and len(child_test) >= 1:\n",
    "                list_child_trainable[child] = {\"train\" : list(child_train), \"test\": list(child_test)}\n",
    "        \n",
    "        if len(list_child_trainable) != 0:\n",
    "            if not self.check_is_trained(parent):\n",
    "                self.train_one_node(parent, list_child_trainable, list_dir_node)\n",
    "        \n",
    "                \n",
    "        if os.path.exists(os.path.join(PATH_CELL_TYPE_DATA, parent)):\n",
    "            with open(os.path.join(PATH_CELL_TYPE_DATA, parent, 'config.json'), 'r') as fin:\n",
    "                data = json.load(fin)\n",
    "            parent_train = [os.path.join(PATH_CELL_TYPE_DATA, parent, 'data_index',x.replace('.hdf5', '.pkl')) for x in data['train']]\n",
    "            parent_test = [os.path.join(PATH_CELL_TYPE_DATA, parent, 'data_index',x.replace('.hdf5', '.pkl')) for x in data['test']]\n",
    "            data_train  = data_train | set(parent_train)\n",
    "            data_test = data_test | set(parent_test)\n",
    "\n",
    "        return data_train, data_test\n",
    "        \n",
    "\n",
    "graph = GraphRandomForest()\n",
    "list_dir_node = {}\n",
    "ans = graph.train_DFS(root, list_dir_node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellSearch",
   "language": "python",
   "name": "cellsearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
